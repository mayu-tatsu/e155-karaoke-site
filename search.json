[
  {
    "objectID": "project_proposal.html",
    "href": "project_proposal.html",
    "title": "Project Proposal",
    "section": "",
    "text": "This project seeks to emulate a karaoke machine by allowing a user to select from a set list of songs and displaying appropriately-timed lyrics accordingly. Once the song has finished, the project will grade the user’s performance — with both a numeric score and letter grade — based on how on-pitch they are.\nTo be more specific, this will be done by sampling the user’s singing with the FPGA and a MEMS PDM microphone, during which the former will perform decimation (see New FPGA Functionality) on the digital output of the mic, bringing the sample rate down to 16 kHz. Then, the FPGA will send this processed information to the MCU via SPI communication, so that it can further process such with FFT computations. It will also handle the storage of expected and detected notes throughout each song. Finally, the MCU will communicate with an LCD over I2C to display the corresponding lyrics of whatever song the user has chosen to sing and display the final score of the user’s performance."
  },
  {
    "objectID": "project_proposal.html#project-overview",
    "href": "project_proposal.html#project-overview",
    "title": "Project Proposal",
    "section": "",
    "text": "This project seeks to emulate a karaoke machine by allowing a user to select from a set list of songs and displaying appropriately-timed lyrics accordingly. Once the song has finished, the project will grade the user’s performance — with both a numeric score and letter grade — based on how on-pitch they are.\nTo be more specific, this will be done by sampling the user’s singing with the FPGA and a MEMS PDM microphone, during which the former will perform decimation (see New FPGA Functionality) on the digital output of the mic, bringing the sample rate down to 16 kHz. Then, the FPGA will send this processed information to the MCU via SPI communication, so that it can further process such with FFT computations. It will also handle the storage of expected and detected notes throughout each song. Finally, the MCU will communicate with an LCD over I2C to display the corresponding lyrics of whatever song the user has chosen to sing and display the final score of the user’s performance."
  },
  {
    "objectID": "project_proposal.html#specifications",
    "href": "project_proposal.html#specifications",
    "title": "Project Proposal",
    "section": "Specifications",
    "text": "Specifications\n\nDesign allows the user to choose a song through external hardware\nDesign detects input frequencies between 300 to 2000 Hz\nDesign scores the user’s singing by comparing detected and expected notes\nDesign plays back the expected song through a speaker as reference\nDesign utilizes a pitch and delay media format (as seen in Fur Elise, Lab 4)\nDisplays properly-timed lyrics on an LCD screen\nLCD display does not flicker"
  },
  {
    "objectID": "project_proposal.html#project-management",
    "href": "project_proposal.html#project-management",
    "title": "Project Proposal",
    "section": "Project Management",
    "text": "Project Management\n\nBill of Materials\n\n\n\n\n\n\nPart Name\nPart Number\nQuantity\nPrice\nVendor\n\n\n\n\nAdafruit PDM MEMS Microphone Breakout\nMP34DT01-M\n1\n$11.23 ($4.95 part, $6.28 shipping + tax)\nAdafruit\n\n\nHosyond I2C 2004 LCD Module\nHD44780U\n1\n$26.32 ($16.99 part, $6.99 shipping, $2.34 estimated tax)\nAmazon\n\n\nCapacitor\n100nF Ceramic Capacitor\n1\n$0\nStockroom\n\n\nTOTAL\n\n\n$37.55\n\n\n\n\n\n\nTable 1: Bill of materials\n\n\n\nNote that if the team discovers that they need any additional, basic resources (such as resistors, capacitors, etc.) later on in the project, these will be taken from either the Engineering Stockroom or Digital Lab at no cost.\n\n\nTimeline\n\n\n\n\n\n\n\n\n\n\n\n\nTask(s)\nDate\n\n\n\n\nProject proposal\nFinish project proposal\n10/16/25\n\n\n\nStart project\n10/30/25\n\n\n\nStudy PDM to PCM decimation\n11/6/25\n\n\n\nImplement new decimation FPGA functionality\n11/7/25\n\n\n\nDetect all input notes accurately with microphone\n11/16/25\n\n\nMidpoint report and demo\nFinish midpoint report and prepare demo\n11/20/25\n\n\n\nSynchronize LCD output notes/lyrics with song(s)\n11/27/25\n\n\n\nPolish off project’s final appearance\n12/3/25\n\n\nFinal checkoff\nFinish project completely\n12/5/25\n\n\nFinal report\nFinish final report\n12/7/25\n\n\nDemo day\n\n12/8/25\n\n\n\n\n\nTable 2: Rough timeline with key dates\n\n\n\n\n\nTask Delegation\n\n\n\n\n\n\n\n\n\n\nMayu\nQuinn\n\n\n\n\n\nStudy PDM to PCM decimation\n\n\nStudy PDM to PCM decimation\n\n\n\n\nStart FPGA code\n\n\nStart LCD code\nTranscribe song(s)\n\n\n\n\nCheck LCD code\n\n\nCheck FPGA code\n\n\n\n\nStudy FFT calculations and libraries\n\n\nStudy FFT calculations and libraries\n\n\n\n\nDecide on interrupts and timers for MCU\n\n\nDecide on interrupts and timers for MCU\n\n\n\n\nAssemble circuit\n\n\nCheck circuit\n\n\n\n\nWrite verification\n\n\nWrite verification\n\n\n\n\nWrite Documentation\n\n\nWrite Documentation\n\n\n\n\n\n\nTable 3: Task delegation\n\n\n\nOverall, the team hopes to complete the vast majority of the project’s tasks together, so as to receive an equal and holistic learning experience."
  },
  {
    "objectID": "project_proposal.html#design-details",
    "href": "project_proposal.html#design-details",
    "title": "Project Proposal",
    "section": "Design Details",
    "text": "Design Details\n\nNew MCU Functionality\nThe new MCU functionality intended to be used is the DMA peripheral to onload buffered data for the PCM data coming in from the FPGA through SPI protocol.\n\n\nNew FPGA Functionality\nThe new FPGA functionality intended to be used is decimation. In short, decimation is a term in (digital) signal processing that describes the removal of samples so as to reduce the complexity of subsequent computations.\n\n\nNew Non-Trivial Hardware\nThe two new, non-trivial pieces of hardware that will be used in this project are the Hoysond LCD module and the Adafruit PDM microphone.\n\n\nRiskiest Element\nThe riskiest element of the project will be the microphone. We are not sure how accurate it will be able to process sounds and output a solid PDM. It is also important to note that we will be demoing the design during a noisy lab room, so we are expecting difficulty in isolating the singing. We will attempt to mitigate the risks by implementing a sigma-delta modulator that we can control via switch."
  },
  {
    "objectID": "project_proposal.html#technical-documentation",
    "href": "project_proposal.html#technical-documentation",
    "title": "Project Proposal",
    "section": "Technical Documentation",
    "text": "Technical Documentation\n\nBlock Diagram\n\n\n\n\n\n\nFigure 1: Block diagram\n\n\n\nThe block diagram depicted in Figure 1 provides a general outline of all protocols and interfaces that the project will comprise.\n\n\nCalculations of Critical Parameters\n\nPDM to PCM Decimation Parameters (Filter Design)\nPage four of the MP34DT01-M datasheet gives the following values:\nClock (@ \\(Vdd=1.8V\\), \\(T=25\\degree C\\)) = 2.4 MHz (Min. 1 MHz, Max. 3.25 MHz)\nAOP (Acoustic Overload Point) = 120 dBSPL\nSNR (Signal-to-Noise Ratio) = 61 dB\nDynamic Range = SNR + (AOP - 94 dbSPL) = 61 dB + (120 dbSPL - 94 dbSPL) = 87 bB\n- Assuming that there is ~6 dB per bit (realistically 6.2 per bit but rounding down), the design will need a PCM word size of at least 15 bits to cover the full dynamic range of the microphone. (\\(87/6=14.5\\), round up to 15 bits)\nThus, the values the team decided on are:\nOutput Sample Rate: 16 kHz → Bandwidth / Nyquist: 8 kHz\n- Most fundamental frequencies and harmonics of human singing fall below 8 kHz, and setting the Nyquist value at 16 kHz comfortably covers this band. - This has a lower processing and memory cost compared to standard 48 kHz audio.\n- FFT frequency bins and time resolution are balanced at this sample rate for pitch detection and scoring (see below!).\nPDM Sample Rate: 2.304 MHz\n- Based on clock range: 1 - 3.25 MHz\n- Power of 2: Important for decimation ratio –&gt; Multi-stage decimation.\nDecimation Ratio: \\(2304 kHz / 16 kHz = 144\\)\n\n\n\n\n\n\nFigure 2: Frequency response diagram\n\n\n\nObserving the diagram provided in Figure 2 above, after 6 kHz, the frequency response starts becoming a lot more sensitive. It’s a pass until then, and then the transition band starts from there. Beyond 10 kHz, the frequency response data isn’t displayed, so assuming the worst case scenario, the stop band should start from there.\nPass-band: 0 - 6 kHz\nStop Band: 10 kHz\nSNR of Output Signal: 80 dB\n- Humans can’t detect signals beyond 70 dB. - Allowing for increased SNR deterioration (of around 7 dB from 87 dB), the team can balance out the computational usage to quality required for a karaoke machine.\nMax. Ripple: +/- 1 dB\n- Individual filters will need to have tighter specs than this overall maximum 1 dB ripple, so for less load, it’s relaxed compared to the high quality audio standard of 0.1 dB.\nStop Band Attenuation = 60 dB\n- Worst-case assumption: PDM noise above 8 kHz (Nyquist) = −40 dB per 16 kHz band\n# all Hz\nfPDM      = 2.304e6\nfPDM/2    = 1.152e6\nfs        = 16e3\nfs/2      = 8e3\n\nN_bands   = floor((1.152e6 - 8e3)/16e3) = 71\n\nSNR_before   = 87   # dB\nSNR_after    = 80   # dB\ndecimation_r = fPDM / fs = 144\n\nP_signal                      = 1                           # normalized signal power\nP_native_noise_before         = 10^(-87/10) = 1.9953e-9     # intrinsic mic noise power\nP_total_noise_allowed         = 10^(-80/10) = 1.0000e-8     # total noise after filtering\nP_aliasing_noise_allowed      = P_total_noise_allowed - P_native_noise_before\n                              = 8.0047e-9   # max allowed aliased noise after filtering\n\n# per-band assumption: -40 dB  (per 16-kHz band above 8 kHz)\nP_aliasing_noise_before_per_band = 10^(-40/10) = 0.0001\nP_aliasing_noise_before_total    = 0.P_aliasing_noise_before_per_band * N_bands = 0.0071\n\nA_stop_lin = 0.0071 / 8.0047e-9 = 8.873e5\nA_stop_db  = 10 * log10(A_stop_lin) = 59.5               # ~60 dB stop-band attenuation required \n\nLUT Usage\nPreliminary order-of-magnitude calculations are based on this blog, but will be tweaked later: Design of a Multi-Stage PDM to PCM Decimation Pipeline\nFollowing the 144 decimation ratio, CIC can handle a maximum of 12x decimation (the bulk of decimation) with four stages and has 0.056 dB passband ripple and 125.8 dB stopband attenuation. The team will use two cascaded half-band filters: HB1 with 4x decimation and HB2 3x. Each roughly has a ~0.02 dB passband ripple and 89 dB attenuation. At the end, a generic FIR filter minimizes ripple by having a flat passband that counteracts the CIC passband droop (“compensation filter”). It also enforces stopband attenuation to a value higher than 60 dB (already achieved!), and as with the role of an FIR filter, it has 1x decimation.\n\nCIC Filter (12x in 4 stages):\n\nUses only adders and delays, no multipliers.\nAdders: 1 LUT per 4-5 adder bits, 20 adder bits * 4 stages = 20 LUTs\n\n20 adder bits comes from = Input Bit Width + Stages * log_2(CIC Decimation Ratio) = \\(1+4*log_2(12) = 16\\), adding 4 guard bits.\n\nRegisters: 52 registers (FFs) where 1 FF = 1 LUT.\n\nComes from 4 stages, each with 1 integrator and 12 combs = 52 LUTs.\n\nOverhead: 20% of preliminary cost = 15 LUTs\nTotal: Roughly 90 LUTs\n\nHalf-Band Filters:\n\nUses multipliers but FPGA can optimize and conver them to shifters/adders since half of taps are zero & nonzero coefficients are usually multiples of 2.\nHB1: 4x\n\nConverting taps to shift-adding is most efficient, but some might not always be possible. Worst case scenario, all taps will be via multiplication.\nAdders: 64 total taps –&gt; 32 nonzero coefficients, where each shift/adder is around 5 LUTs = 160 LUTs\nMultiplication: Worst case all taps are required to use multiplication: we use 4 DSP blocks (out of 8) for HB1, 28 multiplications will need to be done via LUT. 15–18 bits × 15–18 bits multiplication (15-bit PCM word size, but give or take) requires roughly 50 LUTs –&gt; 28*50=1400 LUTs\nTotal: 1400 LUTs at worst case\n\nHB2: 3x\n\nAdders: 48 total taps –&gt; 24 nonzero coefficients, where each shift/adder is around 5 LUTs = 120 LUTs\nMultiplication: Using the last 4 DSP blocks, 24-4=20 multipliers left –&gt; 20*50=1000 LUTs\nTotal: 1000 LUTs at worst case\n\n\nFIR Filter:\n\nAll error calculations are met, so FIR can be made up of precise and small number of taps. Overall, the multiplication count will be small.\nAdding: Around 16-32 taps, where 2-4 LUTs per tap = 248 LUTs\n\nLess LUTs per tap becuase of lower sample rate after decimation\n\nMultiplication: No DSP blocks, 50 LUT per multiplication –&gt; 50 * 32 = 1600 LUTs\nOverhead: 20% of preliminary cost = 300 LUTs\nTotal: 1900 LUTs\n\nOverall Total: 4400 LUTs out of iCE40UP5K’s 5280 LUTs available –&gt; 83%.\n\nA little high, but this is considering worst case; in reality, we will be implementing shift-adding as first priority (rather than multiplication).\nUsing all 8 DSP blocks provided by iCE40UP5K.\nWe can decrease LUT usage by decreasing tap counts as long as our error margins allow it.\nWe can allocate DSPs within Lattice Radiant to handle the largest or tightest coefficients instead and decrease LUT usage.\n\n\n\n\n\nFFT Parameters\nSample Rate (from FPGA’s decimation) [\\(f_S\\)]: 16 kHz –&gt; Bandwidth = 8 kHz\nFFT Size [\\(N\\)]: 1024; balanced choice from Table 4 below\nFrequency resolution per FFT bin:\n\n\\(f_S / N\\) = Sample Rate / FFT Size\nThe team is aiming for a range between 5-20 Hz/bin for musical range\n\nTime resolution (Frame duration):\n\n\\(N / f_S\\) = FFT Size / Sample Rate\n50-100 ms latency is slowest without human notification\n\n\n\n\n\n\n\n\n\n\n\n\nFFT Size\nFrequency Resolution\nTime Resolution\n\n\n\n\n512\n15.625 Hz/bin\n0.064 s = 64 ms\n\n\n1024\n7.8125 Hz/bin\n0.128 s = 128 ms\n\n\n2048\n3.90625 Hz/bin\n0.256 s = 256 ms\n\n\n\n\n\nTable 4: FFT Size Options"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "E155 Karaoke Final Project",
    "section": "",
    "text": "Quinn Miyamoto (left) is a junior engineering major at Harvey Mudd College. More specifically, she is interested in digital electronics and mechanical design. So far, she has completed courses pertaining to computer architecture, electricity and magnetism, and system-on-chip design. She has a Level Two CAD certification in Rhino, and works as a Makerspace steward for Harvey Mudd College, with a certification to teach users welding. When she is not working, you can find her reading manga/manhwa, binging bad Netflix shows, gaming, or rock-climbing.\nMayu Tatsumi (right) is a junior studying Engineering at Harvey Mudd College. She is interested in working in the camera and lighting industries and would like to become involved as an electrical engineer. She has prior experience working as a Systems Control intern specifically working on embedded systems and writing C firmware on the NXP i.MX8M processor. Outside of engineering, she is interested in movie & TV production, Formula 1, and gaming."
  },
  {
    "objectID": "index.html#about-the-team",
    "href": "index.html#about-the-team",
    "title": "E155 Karaoke Final Project",
    "section": "",
    "text": "Quinn Miyamoto (left) is a junior engineering major at Harvey Mudd College. More specifically, she is interested in digital electronics and mechanical design. So far, she has completed courses pertaining to computer architecture, electricity and magnetism, and system-on-chip design. She has a Level Two CAD certification in Rhino, and works as a Makerspace steward for Harvey Mudd College, with a certification to teach users welding. When she is not working, you can find her reading manga/manhwa, binging bad Netflix shows, gaming, or rock-climbing.\nMayu Tatsumi (right) is a junior studying Engineering at Harvey Mudd College. She is interested in working in the camera and lighting industries and would like to become involved as an electrical engineer. She has prior experience working as a Systems Control intern specifically working on embedded systems and writing C firmware on the NXP i.MX8M processor. Outside of engineering, she is interested in movie & TV production, Formula 1, and gaming."
  },
  {
    "objectID": "final_report.html",
    "href": "final_report.html",
    "title": "Final Report",
    "section": "",
    "text": "This E155 final project sought to create a karaoke machine. The design allows users to choose a song to sing, displaying lyrics and playing the song as a guide via speaker, and ultimately compares the user’s singing to the expected frequencies, displaying both a letter grade and a percent error (between received and expected frequencies) after a song finishes. The main input is a MEMS digital microphone, which goes through digital signal processing and FFT to pick up the correct frequencies at which people speak into the microphone. The LCD screen functions as the principal user interface, directing users to choose songs, displaying lyrics, and presenting grades."
  },
  {
    "objectID": "final_report.html#project-overview",
    "href": "final_report.html#project-overview",
    "title": "Final Report",
    "section": "",
    "text": "This E155 final project sought to create a karaoke machine. The design allows users to choose a song to sing, displaying lyrics and playing the song as a guide via speaker, and ultimately compares the user’s singing to the expected frequencies, displaying both a letter grade and a percent error (between received and expected frequencies) after a song finishes. The main input is a MEMS digital microphone, which goes through digital signal processing and FFT to pick up the correct frequencies at which people speak into the microphone. The LCD screen functions as the principal user interface, directing users to choose songs, displaying lyrics, and presenting grades."
  },
  {
    "objectID": "final_report.html#project-specifications",
    "href": "final_report.html#project-specifications",
    "title": "Final Report",
    "section": "Project Specifications",
    "text": "Project Specifications\n\nDesign allows the user to choose a song through external hardware\nDesign detects input frequencies between 300 to 2000 Hz\nDesign scores the user’s singing by comparing detected and expected notes\nDesign plays back the expected song through a speaker as reference\nDesign utilizes a pitch and delay media format (as seen in Lab 4)\nDisplays properly-timed lyrics on an LCD screen\nLCD display does not flicker"
  },
  {
    "objectID": "final_report.html#bill-of-materials",
    "href": "final_report.html#bill-of-materials",
    "title": "Final Report",
    "section": "Bill of Materials",
    "text": "Bill of Materials\n\n\n\n\n\n\nPart Name\nPart Number\nQuantity\nPrice\nVendor\n\n\n\n\nAdafruit PDM MEMS Microphone Breakout\nMP34DT01-M\n1\n$11.23 ($4.95 part, $6.28 shipping + tax)\nAdafruit\n\n\nHosyond I2C 2004 LCD Module\nHD44780U\n1\n$26.32 ($16.99 part, $6.99 shipping, $2.34 estimated tax)\nAmazon\n\n\nTOTAL\n\n\n$37.55\n\n\n\n\n\n\nTable 1: Bill of materials"
  },
  {
    "objectID": "final_report.html#design-details",
    "href": "final_report.html#design-details",
    "title": "Final Report",
    "section": "Design Details",
    "text": "Design Details\nThe general, high-level overview of our project — with all of the protocols used explicitly outlined — can be viewed in Figure 1 below:\n\n\n\n\n\n\nFigure 1: Design overview\n\n\n\n\nNew Non-Trivial Hardware\nThe two new, non-trivial pieces of hardware that are used in this project are the Adafruit PDM microphone and the Hoysond LCD module.\n\nAdafruit PDM Microphone\nWe used a MP34DT01-M, a MEMS digital microphone. It outputs data in Pulse-Density Modulation (PDM) format, which is a format of representing an analog signal in a single bit, with the density of 1’s and 0’s reflecting the amplitude of the signal.\nThis microphone itself took 4 inputs: a 3.3V power supply, a ground, a L/R channel selector for simplified stereo applications, and a synchronization input clock. The input clock was valid between frequencies of 3.25 to 1 MHz, which meant we were able to select a frequency and drive it from the FPGA. We chose to ground L/R out of default. The output was the Dout pin, and could be read by an FPGA pin in PDM format. The clock and Dout were connected directly to the FPGA board to minimize lag and voltage drop.\nWithin E155, there has been no usage of this PDM format. The closest data format or acquisition method we had gone over was quadrature encoders, which we used in Lab 5. They were able to represent analog motion in two 1-bit digital signals, but PDM is a distinct method of representing analog waves in a single bit, which is beyond the previous material of this course.\n\n\nHoysond LCD Module\nThe LCD used seems to comprise the HD44780 Hitachi controller chip, as well as a PCF8574A bit-expander for I2C interfacing. The screen itself has four rows and 20 columns — or, in other words, 80 total spaces — with which to write characters of its user’s choosing. It needs 5 V of power to display characters properly, with the ability to adjust both its backlight brightness and contrast. There are two options for sending it instructions: either over I2C or directly through its 11-pin parallel interface, including:\n\n1 Enable bit (E)\n1 Register Select bit (RS)\n1 Read/Write bit (RW)\n8 data pins (D0 - D7)\n\nMore specifics about these pins and their respective functionalities can be found in the datasheet for the LCD’s Hitachi controller, specifically. Regardless, for both methods of communication, the module uses the sequences of high and low voltages detected in tandem with a look-up table, with each operation having its own lead-in op-code (such as the Set Address and Clear Display functions) and each character having its own unique sequence associated with it. The latter can be viewed rather clearly in Figure 2, as follows:\n\n\n\n\n\n\nFigure 2: LCD character options\n\n\n\nNote that if you decide to communicate with the LCD by sending bits to all of its pins in parallel, each transaction will have to be ended with a short pulse of the Enable (E) bit, so as to tell the module that its information has been completely updated.\nMoreover, the LCD requires a specifically-timed initialization sequence in order to fully function. This is outlined in Figure 3.\n\n\n\n\n\n\nFigure 3: LCD initialization\n\n\n\nUltimately, the features implemented with this hardware go above and beyond the previous material covered in class, as they require the configuration of an external display. More specifically, we had to learn about the different communication protocols that an LCD can use, and compare the pros and cons of both. Eventually, we came to the conclusion that using a parallel interface would be best because 1) when there are pins to spare for it, it’s the fastest, and 2) the datasheet for the PCF8574A bit-expander is vague and thus, it is extremely hard to parse how to interact with it when using I2C; in the interest of not getting hung-up over something that could be easily avoided, we decided to bypass it entirely. (See the Schematic section below for more details on some bugs that we needed to address when doing this.)\n\n\n\nFPGA Design Overview\nThe FPGA is in control of the microphone, processing its data, and sending that data to the MCU. Our microphone is a MP34DT01-M, a MEMS digital microphone. We drive this using a 1.536 MHz clock signal generated by the FPGA, and the microphone outputs data in Pulse-Density Modulation (PDM) format, which is then fed back into the FPGA. PDM is a format of representing an analog signal in a single bit, with the density of 1’s and 0’s reflecting the amplitude of the Figure 4 signal.\n\n\n\n\n\n\nFigure 4: PDM vs. Analog signals\n\n\n\nThe top module is karaoke_top.sv, as found in the GitHub’s fpga/src. The FPGA feeds the PDM signal through a pipeline of 3 digital audio filters: a CIC filter / cic.sv, half-band filter / hb.sv, and FIR filter / fir.sv.\nThese three filters were used to create a decimation ratio of 96, downsampling the 1.536 MHz PDM output down to a 16 kHz output. The CIC takes the brunt of the decimation with a 24 decimation ratio in 4 stages, converts the PDM to a PCM, and normalizes the output values between -1 to 1. The half-band and FIR filters create a passband from 0 to 4000 Hz, with a stopband at around 8000 Hz. Both filters have a decimation ratio of 2, and their coefficients were determined using matplotlib functions.\nThe three filters generate a 16-bit, 16 kHz PCM output, with a new sample indicated by a pulse. These signals are fed into the SPI module / spi.sv, which generates a CS, SCK, and SDO, acting as the controller/master in this case. All relevant testbenches are found in fpga/testbench and the ModelSim project is found in fpga/work. The MCU then receives these values, acting as the peripheral/slave, and accumulates them using the SPI peripheral, paired with the DMA peripheral.\nSome difficulties we encountered:\n\nFinding the correct filter pipeline design and the coefficients for the half-band and FIR\nMinimizing resource usage\nFiguring out new SPI configuration and implementing it\nSynthesizing and flashing the executable\n\nInitially, we were naively hoping to create a decimation ratio of 144, bringing a PDM input rate of 2.304 MHz down to 16 kHz. This was due to a misunderstanding of half-band filters and the resources we could fit onto the board. Half-band filters are optimized FIR filters that require only half the number of taps, and therefore multiplications, but also can only decimate (or interpolate) at a rate of 2 every filter. We had assumed that half-bands filters would be able to handle 3x and 4x decimation (which, no, is not possible) during the project proposal stage. Once we read up more on half-bands, we decided to move to a 1.536 MHz to decimate it down to 16 kHz using a decimation ratio of 96. We determined that, for our purposes, the CIC filter would be able to handle a 24 decimation ratio without much change in ripple or attenuation via this Figure 5 chart:\n\n\n\n\n\n\nFigure 5: CIC decimation chart\n\n\n\nSince we wanted to make sure that the FIR would be able to handle everything in 16 taps to keep resource levels low, we decided to use two half-bands and keep the FIR only to minimize passband ripple and maximize stopband attenuation. However, when we loaded this onto the FPGA, there were not enough LUTs to accommodate this design. As a result, we settled on a finalized design of a 24-decimation ratio, 4-stage CIC filter, a half-band filter, and a 2x decimating and correcting final FIR filter.\nWhen it came to determining the coefficients, we started off trying to use some MATLAB code, but switched to Python with its easier remez() function. The code for this can be found here. When it came to implementing these filters in Verilog, we mentioned that it was difficult due to resource constraints. There was some trial-and-error involved, trying to get it down from 170+% resource usage down to our final 94% LUT4 usage. Our biggest issue was the multiplications required. The FPGA only has 8 DSP blocks, and all of those were devoted to the half-band multiplications. Which meant that the CIC, FIR, and the rest of the half-band additions all had to fit their calculations in the LUTs. The CIC was generally resource-optimized already due to its perk requiring only adders and delays by design. The FIR and half-band took some tweaking. We implemented pipelining in both filters to keep the critical path short and allow the synthesis tool to map arithmetic into smaller, slower LUT-based adders rather than huge blocks dedicated to multiplication. Then, we replaced all true multipliers in the FIR with hardcoded shift-add coefficient implementations, which finally pushed down our LUT usage below 100%.\nWe also hit some snags while implementing the SPI. We were using some code based on Lab 7, which assumes a MCU-master, FPGA-slave configuration. However, it quickly came to be that our FPGA should be the master in our context, and as we switched those things around, as a result, we needed a new chip select (CS) signal, and switched around some pins. However, there was some confusion about which board should generate the CS signal between us partners, and the silkscreen and boards were generally designed around the assumption that the MCU would always be the master. That, and some two sets of pins coming out of the FPGA both labelled SCK/CS/MISO-CIPO took a day or two to figure out.\nFinally, once we got almost everything working regarding the MCU peripherals and the FPGA SPI module, we had some issues where, even in ambient noise, the microphone, filters, and FFTs combined would produce outputs ranging from 100 to 6000 Hz, jumping values multiple times in a second. We knew from verification and testing that all of these mechanisms should work in isolation, so we weren’t sure where the mix-up was coming from. In a last-ditch test to see if the ambient noise was too much (which it wasn’t), we left the lab to go to a quieter room. It turns out that synthesizing and loading the executable from Quinn’s personal computer through OpenFPGALoader fixed completely all of our issues despite using the same exact code and project setup (same .rdf files for Lattice, same Lattice version). There may be some timing constraints that we never specified that could be causing these mapping inconsistencies, but overall, we are still unsure exactly why this is. However, as a result, the microphone and FFT are now able to accurately detect and output frequencies.\n\n\nMCU Design Overview\nIn short, our MCU serves as the primary driver for everything in this project that isn’t our microphone. The most significant components of its design — comprising both peripheral configurations and general functionalities — are listed and expounded upon as follows (note that a more visual representation of these routines can be found in a later Flowchart section):\n\nSPI\nFirst and foremost, SPI Receive-Only mode is set up such that the FPGA acts as the controller in every transaction, while the MCU acts exclusively as the peripheral. Although we initially used the SPI initialization function provided in Lab 7 as starter code, we quickly realized that this wouldn’t work, due to the nature of what we were trying to accomplish: a one-sided intake of information that the FPGA initiates, as opposed to two-way communication started off by the MCU. Taking advantage of the built-in connections between the FPGA and MCU pins on our PCB, we eventually set up the latter to continuously receive SCK, CS, and SDO signals as input from the former. It then stores the SDO data — our raw audio samples — in its data register, which we configure to be accessible by DMA.\n\n\nDMA\nThe DMA peripheral, in essence, negates the need to use the spiSendReceive() function every time we want to read the data our MCU is receiving. Instead, it offers the option of automatically storing the contents of the SPI data register into a buffer array, so that the CPU is free to focus on running other, more intensive tasks. Notably, we configure our DMA to run in circular mode, so that we don’t need to manually disable and reenable the peripheral every time we want to update the array.\nAdditionally, we have an interrupt that triggers on the DMA’s transfer-complete flag, waits until FFT calculations are complete, and copies the DMA buffer’s contents into a second, FFT-accessible array. This is done so that there is always one buffer holding the raw audio data and one buffer being mathematically processed, such that we don’t lose any important information in the case of the latter process being delayed.\n\n\nAudio\nFor the actual song-playing aspect of the project, we simply reused the code we wrote previously in Lab 4 to play each song’s main singing melody. In fact, Lab 4 is where part of the original idea to do this stems from — we both thoroughly enjoyed the assignment and were interested in pursuing more ventures into the realm of audio. Overall, we mainly took the following: the PWM-generation code (generalized to work with any timer this time) and the specific setup for song arrays, {frequency (Hz), duration (ms)}. However, to achieve cleaner code this time, we decided to store all relevant song information in a header file instead of our main function. At the moment, we have two songs available to be played: Mr. Brightside and Golden.\nIn the future, if we are to work on this project once more, we think it would be nice to have multiple tunes playing at once, so that we can realize the full brilliance of our songs and their harmonies. Moreover, it would be nice if there were a more convenient, less labor-intensive way to create each song array. This would allow us to have more song options available.\nCredit goes to Cole Plepel for doing the actual transcribing of our desired notes.\n\n\nTiming\nTo ensure that a given song’s lyrics show up at the same time as the relevant beats, we first made an array that contained strings we could pass into the display_message() function (elaborated on below). Then, we created a timing array; this is the exact same length as the notes array, populated with zeros for notes we don’t want to change lyrics on and the index of the desired lyric on the notes we do want to change lyrics on. Whenever a nonzero timing array item is detected, it goes to the index of the lyrics array indicated and prints the words accordingly. While this was rather tedious to write and check by hand, it provides a fool-proof way of showing the correct line at the correct time always.\n\n\nFFT\nIn order to understand what each audio sample’s dominant frequency is — and therefore, what note someone is singing — an FFT needs to be applied to the raw values. Thankfully, with the help of a tutorial, this was extremely easy to do. In short, we have downloaded the ARM math library and are now making use of its built-in functions. First, we apply Z-score normalization to the raw data, then a Hanning window, in order to maintain amplitude measurement accuracy while simultaneously reducing spectral leakage. Next, we pass this pre-processed signal into the actual FFT function, and take the magnitude of its resulting frequencies. Finally, we pick out and return the largest one.\n\n\nGrading\nAlthough our song-storing scheme is noted to be quite cumbersome above, it also allows for extremely easy grading. More specifically, we set a flag every time we’re done playing a note, which then triggers an interrupt. Inside the interrupt, we call our FFT function and compare the output frequency of that to the note we just played (and are therefore expecting to also hear from the user) at that point in time. We then keep a running total of the error calculated from this comparison, as well as a running total of the times this interrupt has been triggered.\nAt the very end of a song’s run, we proceed to call singing_grader(), which calculates the user’s average error over the entire song by dividing the aforementioned running total of frequency error over the number of interrupt triggers, then multiplying the quotient by one hundred. Finally, we use the LCD to provide a letter grade based on — quite generously — a curve, as well as the percent error computed.\n\n\nLCD\nThere are five significant functions for the LCD: lcd_display_initialization(), lcd_display_write(), character_converter(), lcd_display_reset(), and display_message().\nThe first, lcd_display_initialization(), is rather self-explanatory. As mentioned in the New, Non-Trivial Hardware section above, there is a very specifically timed process that the LCD needs to go through in order to properly display anything. While it was a little hard to find the relevant page in the datasheet at first, we were eventually able to accomplish this easily. It also turns off the cursor and sets the starting address of the LCD’s Write to the first row and first column.\nlcd_display_write() is also a simple function. It receives a nine-bit integer input and shifts each bit into the appropriate pin, such that the LCD is given some high-low sequence that it recognizes. E is then pulsed to push the updated information fully.\nMeanwhile, character_converter() consists entirely of one giant look-up table that matches the one depicted in Figure 2 above. It takes a string as input and checks whether or not said string matches a known character. If that is indeed the case, it outputs the corresponding number sequence that the LCD will actually understand.\nFurthermore, lcd_display_reset() turns the display off, clears the screen, and turns it back on again.\nFinally, display_message() combines all of the above to take in longer strings — such as lyrics — and get the LCD screen to output them accordingly. Note that due to the physical restrictions of the display, with there only being space for 80 characters at a time, writing more than that alloted amount will lead to wrap-around and incorrect messages; also, inputs have to be formatted taking into account the fact that the LCD writes to the first row first, then the third, second, and fourth, in that order. We initially struggled a little with stitching everything together, due to not really understanding pointers and addresses as well as we should have, but after addressing our mistake of passing a pointer into the function instead of a regular string like it was expecting, everything turned out fine.\n\n\nSong Selection\nSongs are chosen when you flip the associated switch to enable them. The LCD tells you which switch is which."
  },
  {
    "objectID": "final_report.html#technical-documentation",
    "href": "final_report.html#technical-documentation",
    "title": "Final Report",
    "section": "Technical Documentation",
    "text": "Technical Documentation\nThe source code for this project can be found in the associated GitHub repository.\n\nBlock Diagram\n\n\n\n\n\n\nFigure 6: Block diagram\n\n\n\nThe block diagram in Figure 6 depicts the general architecture implied by the SystemVerilog code.\nOur inputs are the reset on the dev board and the continuous single-bit stream of data, pdm_in, from the microphone. The clk_gen module generates the internal 1.536 MHz clock and the 6 MHz clock that drives the 3 MHz SPI. The pdm_data input is first put into a synchronizer just in case since our leads are long (technically, the clk going out to the microphone should already have made sure of synchronization). The output data goes to the CIC (cic.sv), where it is transformed into a 16-bit, 64 kHz PCM with a validity signal that pulses whenever a new value is available. Those two signals then pass into the half-band filter (hb.sv), where it transforms into a 32 kHz PCM with a specific passband and stopband, along with some passband ripple and stopband attenuation. Then, these values are passed along to the final filter, the FIR (fir.sv), where it is finally decimated by another factor of 2 down to 16 kHz, and its ripple and stopband attenuation are corrected. The 16-bit values and the validity signal are finally passed into the SPI (spi.sv), where a sck, sdo, and cs signal are generated every 16 kHz on a 3 MHz SPI signal using cross-domain clocking. (TLDR: A new 16-bit PCM value is shifted out from the MSB to the LSB on the 3 MHz clock, with the FPGA acting as the master and the MCU as the slave.)\n\n\nSchematic\n\n\n\n\n\n\nFigure 7: Schematic\n\n\n\nFigure 7 above depicts the physical connections between all of the project’s necessary electronic components. This includes the PDM microphone, LCD screen, speaker, audio amplifier, switches, FPGA, and MCU.\nNote that the microphone and FPGA interfacing is done in accordance with the MP34DT01-M datasheet. The most prominent feature of note is the tying of the LR port to ground, as that configures a pdm_data signal pattern of valid data when the CLK signal is low and high impedance when CLK is high.\nAdditionally, the MCU has direct, one-to-one pin connections with the LCD, with the exceptions being RW and V0. RW is tied directly to ground because we are only performing Write operations with the LCD and never Read ones, so the bit never needs to be set; by just wiring it to ground instead of sending a zero to the pin every time, this allows us to free up MCU ports that we can use for other, more important connections. Meanwhile, V0 is wired to the output of a 10 kΩ potentiometer, so that the contrast between the backlight and the characters being displayed can be controlled.\nFurthermore, the MCU-audio-amplifier-speaker circuit shown at the very bottom is yet another component taken from Lab 4, affording the users the ability to modify the speaker’s volume.\nHowever, most notably of all, is the fact that MCU pins PA5 and PB7 are no longer connected, nor are PA6 and PB6. Previously, while it is not noted on the PCB schematic we are given for this class — or anywhere easily found in official STM32 documentation, for that matter — those two pairs of pins share respective connections via two surface-mounted 0 Ω resistors. This was rather frustrating to discover, as we were struggling with the LCD showing randomly inserted, completely incorrect characters for the longest time. As it turns out, because the LCD (without using I2C to communicate) depends entirely on the accuracy of analog signals, it was confused when it received voltages of 1.5 from those four pins, specifically — half of the 3.3 V signal it should have been getting. Seeing as 1.5 V is below the minimum threshold of 2.2 V to count as a high signal and above the maximum threshold of 0.6 V to count as a low signal, the LCD simply chose at random every time it encountered this problem. Thus, while difficult to portray in the above schematic, the removal of these resistors is perhaps the single-most important aspect of the circuit design to our team.\n\n\nFlowchart\n\n\n\n\n\n\nFigure 8: Flowchart\n\n\n\nThe Figure 8 flowchart provides a detailed overview of the microcontroller’s most significant routines."
  },
  {
    "objectID": "final_report.html#results",
    "href": "final_report.html#results",
    "title": "Final Report",
    "section": "Results",
    "text": "Results\n\nDesign allows the user to choose a song through external hardware\n\nThe red switch on the breadboard allows for choosing between Mr. Brightside (switch 4) and Golden (switch 2) after boot, and the LCD screen displays the instructions / options if no song is selected.\n\nDesign detects input frequencies between 300 to 2000 Hz\n\nThe microphone is able to detect frequencies between 250 Hz to 2400 Hz with a resolution of around 25 Hz. There is some latency when displaying detected frequencies on the LCD [in enable_test=1 mode], mostly as a result of the LCD. It is much faster when printing in SEGGER in debug mode, though that is also bottle-necked by the printf function. For a higher resolution, a higher FFT length could be used, though it would result in a higher latency as well. The testbenches for the filters used to make this possible is displayed below.\n\nDesign scores the user’s singing by comparing detected and expected notes\n\nThe design outputs a final score after averaging the amount of percent error based on each note expected. This is achieved by accumulating the percent error of each note and dividing the total at the end by the number of samples taken. We have implemented a lenient grading system as a result of the generally loud ambient noise in the Digital Lab and accounting for the talking during Demo Day, as found in this file.\n\nDesign plays back the expected song through a speaker as reference & Design utilizes a pitch and delay media format (as seen in Lab 4)\n\nRegarding both specs: we had to transcribe both songs, Golden and Mr. Brightside, into the Lab 4 {frequency, length} format. This code can be found here. The PWM and delay timer initialization code were similar to the lab code as well.\n\nDisplays properly-timed lyrics on an LCD screen\n\nThere is a LUT to pass in the proper signal to display a single character, and these characters\n\nLCD display does not flicker\n\nYes, you can see this in this demo video.\n\nTestbench Simulation\n\n\n\n\n\n\nFigure 9: CIC filter testbench\n\n\n\nAn PDM input with various 1/0 densities represents a +1, -1, 0, sin(500), sin(5000), sin(15000), and sin(30000), along with a impulse, step response, and a mixed signal of a combined 1000, 8000, and 25000 Hz. You can see that the 1-bit signal is converted into a 16-bit, 64 kHz signal with the 15 kHz and 30 kHz attenuated out, as expected.\n\n\n\n\n\n\nFigure 10: Halfband filter testbench\n\n\n\nYou can see that the validity signal, passed in at 64 kHz, is decimated by 2 down to 32 kHz (compare x_in_valid and y_out_valid). You can also see the same in the signals that are passed through, along with some latency.\n\n\n\n\n\n\nFigure 11: FIR filter testbench\n\n\n\nAgain, you can see that the validity signal gets decimated by 2 down to 16 kHz and that the output signal gets choppier as a result. Also some attenuation occurs at frequencies of 7 kHz and 14 kHz, which is as expected.\n\n\n\n\n\n\nFigure 12: SPI testbench\n\n\n\nWe passed in values through pcm_out, and you can see the correct values being passed through SCK (though only 0x0000, 0xFFFF are easy to read). The SPI runs at 3 MHz, but realistically, after the three filters, expected outputs come out at a frequency of 16 KHz, which is the width between inputs of 0x1234, 0x5678, and 0x9abc (so you would see this speed on the logic analyzer being fed into the MCU).\n\n\nOscilloscope Traces\n\n\n\n\n\n\nFigure 13: LCD Logic Analyzer traces\n\n\n\n\n\nDocumentation\n\n\n\n\n\n\nFigure 14: Demo video\n\n\n\n\n\n\n\n\n\nFigure 15: Final build\n\n\n\n\n\n\n\n\n\nFigure 16: Team on Demo Day"
  },
  {
    "objectID": "final_report.html#references",
    "href": "final_report.html#references",
    "title": "Final Report",
    "section": "References",
    "text": "References\nE155: Course website.\nSTM32L432KC Datasheet: STM32 MCU datasheet.\nSTM32L432KC Reference Manual: STM32 MCU reference manual.\nQuinn Miyamoto’s E155 Portfolio: Team member Quinn Miyamoto’s portfolio of this course’s labs.\nFix That Note! E155 Final Project Portfolio: Past E155 portfolio for the Fix That Note! project, which involved telling a user what note their system was detecting.\nAn Intuitive Look at Moving Average and CIC Filters: A series of blogs made by Tom Verbeure on designing a pipeline for audio decimation and filtering.\nFIR Halfband Filter Design: A MATLAB guide to designing half-band filters, used in conjunction with the DSP-Related article below.\nSimplest Calculation of Half-band Filter Coefficients: An online guide to designing half-band filter coefficients.\njaycordaro/half-band-filter: An implementation of a half-band FIR filter, from MATLAB to fixed point in SystemVerilog.\nMEMS audio sensor omnidirectional digital microphone: Datasheet for MP34DT01-M.\nInterface 16×2 LCD with STM32 (No I2C) — 4-bit Guide: A guide to interfacing an LCD with an STM32 MCU with an 11-line parallel bus, instead of I2C.\nSTM32 LCD 16×2 Library & Example | LCD Display Interfacing: A guide to initializing and writing to an LCD using an STM32 MCU.\nI2C Serial Interface 20x4 LCD Module: Datasheet for the LCD’s HD44780 controller chip.\nMC21605A6W-FPTLW: Alternative LCD datasheet.\nGenerating signals with STM32L4 timer, DMA and DAC: A guide to interfacing DMA with SPI.\nFast Fourier Transform using the ARM CMSIS Library within the STM32 MCUs: A video tutorial on how to pick a dominant frequency out of a given signal on the STM32 MCU, using ARM math functions (including the FFT).\nNormalizing Signals for Enhanced Analysis and Interpretation: A guide to normalizing signals as preparation for Fourier Transforms.\nSTMicroelectronics/cmsis-core: GitHub repository of all ARM math functions."
  },
  {
    "objectID": "final_report.html#acknowledgements",
    "href": "final_report.html#acknowledgements",
    "title": "Final Report",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nWe would like to thank the entire MicroPs teaching team for their help in getting this project off of the ground this semester, especially Kavi Dey (’26) and Prof. Spencer. Additionally, special thanks goes out to Cole Plepel (’27) for transcribing our karaoke machine’s music. We really appreciate you!"
  },
  {
    "objectID": "midpoint_report.html",
    "href": "midpoint_report.html",
    "title": "Midpoint Report",
    "section": "",
    "text": "As of this project’s midpoint checkoff (11/20/25), the microphone is somewhat functional and sends audio samples to the FPGA board as desired. The FPGA then performs decimation on the signal by applying one CIC filter, one half-band filter, and one FIR filter, in that order. The CIC filter first decimates the 1.536 MHz PCM input by a factor of 12 through 4 stages to output a 64 kHz 16-bit PCM. The 4 initial integrator blocks accumulate the PDM stream while the decimator takes one out of 12 values. The 4 combs then compute successive differences between delayed samples, which means that they effectively undo the integrator buildup and restore a band-limited approximation of the original signal at the lower sample rate. The half-band filter is a fancy and more efficient FIR filter, using the properties of specific coefficients to halve the number of multiplications and make it more resource-efficient. The coefficients for the half-band were found by specifying a desired passband of 3950 Hz and stopband of 7900 Hz, along with a set decimation ratio of 2 (as half-bands are hardcoded to have) and number of taps, and passed all those parameters into the remez Python function. Essentially, we used this half-band filter to lower the resource strain and still decimate by 2 so we went from a 16-bit 64 kHz PCM to a 32 kHz 16-bit PCM. The FIR filter coefficients were found by calculating the passband ripple and stopband attenuation that results from the first CIC and halfband filters, and specifying the targeted values (around +- 3dB ripple and 60 dB attenuation at least). The FIR filter also had a decimation ratio of 2 to bring down the final frequency to 16 kHz (with a Nyquist of 8 kHz) that maintains all signals required ot distinguish tones during singing. All filters have a one-cycle output pulse that is asserted when there is a new valid value available. This is passed in to the SPI module, along with the final 16-bit and 16 kHz decimated PCM values, to be sent to the MCU.\nThis process of figuring out the coefficients was extremely difficult due to differences between what we had planned and what was possible. We had originally planned for a 144 decimation ratio starting from a 2.304 MHz PDM rate, with 2 halfbands (x4, x3) in between the CIC (x12) and compensation FIR filter (x1). However, we were not aware that halfbands were only as efficient as they are when they are at x2 decimation. Then, given the limited resources on the FPGA and the slowly mounting number of LUTs as we implemented the filters, we came to the decision that only a single halfband (x2) filter would be possible. Our final plan was the 12x, 4-stage CIC filter, than a single halfband (x2) filter, and the final compensation FIR filter that also decimated by 2. Every single iteration of this pipeline required re-calculating feasible coefficients for the halfbands and FIRs, which took effort and learning of a new section of the matplotlib library in Python. It also took a while implementing a pipeline on the halfband and FIR filters, along with shift-adding to reduce multiplication and, in extension, resource usage.\nAs the FPGA sends these values to the MCU over SPI, where it proceeds to first apply Z-Score normalization with the intent of maintaining amplitude measurement accuracy; it is also adjusted to account for the CIC filter gain. Note that the MCU is configured in such a way that the SPI transactions automatically send data directly to a buffer in memory via the DMA peripheral, so as to reduce the amount of processing that the core itself needs to do. Next, a Hanning window is used to reduce the signal’s spectral leakage, after which it utilizes the ARM math library functions to actually apply the FFT and take the magnitude of the resulting frequencies. From this, the dominant frequency is found and subsequently output as a note (ex. A3, B4, C5, etc.) in Segger’s Debug terminal. Additionally, while this is all occurring, a data-transfer-complete flag and an FFT-complete flag are constantly being set and reset, so that the program knows when it can start the next round of computations. (See the Flowchart section below for more specific details about the MCU’s general routine and functions.)\nFrom here on out, the team will need to get the LCD and speaker(s) working and properly interfaced with the current system, such that various songs and their lyrics can be played and displayed, respectively. There will also need to be some sort of song-selection mechanism integrated into the process. Moreover, the team has a few timing/latency concerns — regarding the delay between input audio signals and their processed outputs, in particular, as displayed in the verification waveforms below — that will need to be addressed. However, because the riskiest element of the project has already been successfully implemented, completing the rest of these tasks should not be as big of a problem."
  },
  {
    "objectID": "midpoint_report.html#current-status",
    "href": "midpoint_report.html#current-status",
    "title": "Midpoint Report",
    "section": "",
    "text": "As of this project’s midpoint checkoff (11/20/25), the microphone is somewhat functional and sends audio samples to the FPGA board as desired. The FPGA then performs decimation on the signal by applying one CIC filter, one half-band filter, and one FIR filter, in that order. The CIC filter first decimates the 1.536 MHz PCM input by a factor of 12 through 4 stages to output a 64 kHz 16-bit PCM. The 4 initial integrator blocks accumulate the PDM stream while the decimator takes one out of 12 values. The 4 combs then compute successive differences between delayed samples, which means that they effectively undo the integrator buildup and restore a band-limited approximation of the original signal at the lower sample rate. The half-band filter is a fancy and more efficient FIR filter, using the properties of specific coefficients to halve the number of multiplications and make it more resource-efficient. The coefficients for the half-band were found by specifying a desired passband of 3950 Hz and stopband of 7900 Hz, along with a set decimation ratio of 2 (as half-bands are hardcoded to have) and number of taps, and passed all those parameters into the remez Python function. Essentially, we used this half-band filter to lower the resource strain and still decimate by 2 so we went from a 16-bit 64 kHz PCM to a 32 kHz 16-bit PCM. The FIR filter coefficients were found by calculating the passband ripple and stopband attenuation that results from the first CIC and halfband filters, and specifying the targeted values (around +- 3dB ripple and 60 dB attenuation at least). The FIR filter also had a decimation ratio of 2 to bring down the final frequency to 16 kHz (with a Nyquist of 8 kHz) that maintains all signals required ot distinguish tones during singing. All filters have a one-cycle output pulse that is asserted when there is a new valid value available. This is passed in to the SPI module, along with the final 16-bit and 16 kHz decimated PCM values, to be sent to the MCU.\nThis process of figuring out the coefficients was extremely difficult due to differences between what we had planned and what was possible. We had originally planned for a 144 decimation ratio starting from a 2.304 MHz PDM rate, with 2 halfbands (x4, x3) in between the CIC (x12) and compensation FIR filter (x1). However, we were not aware that halfbands were only as efficient as they are when they are at x2 decimation. Then, given the limited resources on the FPGA and the slowly mounting number of LUTs as we implemented the filters, we came to the decision that only a single halfband (x2) filter would be possible. Our final plan was the 12x, 4-stage CIC filter, than a single halfband (x2) filter, and the final compensation FIR filter that also decimated by 2. Every single iteration of this pipeline required re-calculating feasible coefficients for the halfbands and FIRs, which took effort and learning of a new section of the matplotlib library in Python. It also took a while implementing a pipeline on the halfband and FIR filters, along with shift-adding to reduce multiplication and, in extension, resource usage.\nAs the FPGA sends these values to the MCU over SPI, where it proceeds to first apply Z-Score normalization with the intent of maintaining amplitude measurement accuracy; it is also adjusted to account for the CIC filter gain. Note that the MCU is configured in such a way that the SPI transactions automatically send data directly to a buffer in memory via the DMA peripheral, so as to reduce the amount of processing that the core itself needs to do. Next, a Hanning window is used to reduce the signal’s spectral leakage, after which it utilizes the ARM math library functions to actually apply the FFT and take the magnitude of the resulting frequencies. From this, the dominant frequency is found and subsequently output as a note (ex. A3, B4, C5, etc.) in Segger’s Debug terminal. Additionally, while this is all occurring, a data-transfer-complete flag and an FFT-complete flag are constantly being set and reset, so that the program knows when it can start the next round of computations. (See the Flowchart section below for more specific details about the MCU’s general routine and functions.)\nFrom here on out, the team will need to get the LCD and speaker(s) working and properly interfaced with the current system, such that various songs and their lyrics can be played and displayed, respectively. There will also need to be some sort of song-selection mechanism integrated into the process. Moreover, the team has a few timing/latency concerns — regarding the delay between input audio signals and their processed outputs, in particular, as displayed in the verification waveforms below — that will need to be addressed. However, because the riskiest element of the project has already been successfully implemented, completing the rest of these tasks should not be as big of a problem."
  },
  {
    "objectID": "midpoint_report.html#midpoint-specifications",
    "href": "midpoint_report.html#midpoint-specifications",
    "title": "Midpoint Report",
    "section": "Midpoint Specifications",
    "text": "Midpoint Specifications\n\nProficiency\n\nClear and concise written summary of current status\nSchematics of all breadboarded circuits\nBlock diagram of system components and the interfaces between them\nHardware demo functioning (potentially with some minor bugs)\nMore than 25% of the way to the final deliverable\nClear description of microcontroller routines\nAll external parts ordered\n\nPDM microphone\nLCD\n\n\n\n\nExcellence\n\nAll interfaces in the block diagram defined (e.g., SPI, I2C, GPIO)\nHardware demo functioning well\nWriting is well organized\nSolid grammar or spelling issues (no more than a few minor errors which do not detract from the point).\nCompleted riskiest element of the project\n\nFPGA implements all three filters to perform decimation appropriately\nMicrophone interfaces with the FPGA and MCU\n\nMore than 50% of the way to the final deliverable"
  },
  {
    "objectID": "midpoint_report.html#technical-documentation",
    "href": "midpoint_report.html#technical-documentation",
    "title": "Midpoint Report",
    "section": "Technical Documentation",
    "text": "Technical Documentation\nThe source code for this project can be found in the associated GitHub repository.\n\nBlock Diagram\n\n\n\n\n\n\nFigure 1: Block diagram\n\n\n\nThe block diagram in Figure 1 depicts the general architecture implied by the SystemVerilog code.\n\n\nSchematic\n\n\n\n\n\n\nFigure 2: Schematic\n\n\n\nFigure 2 above depicts all of the components comprising the breadboarded circuit at the time of the midpoint checkoff. This includes the PDM microphone, FPGA, and MCU; note that while the connections between the FPGA and MCU did not need to be explicitly outlined, as they are both on-board elements of the PCB that are hooked up internally, these were shown for the sake of clarity, especially regarding the SPI transactions. Overall, the microphone and FPGA interfacing was done in accordance with the MP34DT01-M datasheet. The most prominent feature of note is the tying of the LR port to ground, as that configures a DOUT signal pattern of valid data when the CLK signal is low and high impedance when CLK is high.\n\n\nFlowchart\n\n\n\n\n\n\nFigure 3: Flowchart\n\n\n\nThe Figure 3 flowchart provides a detailed overview of the microcontroller’s most significant routines.\nFirst and foremost, the main loop’s primary jobs are to 1) configure the MCU and all of its relevant peripherals, 2) continuously apply an FFT to the detected audio samples — which are, again, transmitted to the controller via SPI — in order to determine the dominant frequencies, and 3) output the corresponding note. This notably requires the help of two custom functions, frequency_determiner and note_determiner. The former makes use of the ARM math library to actually perform the FFT calculations (with some pre-processing work done to ensure greater accuracy and resolution), after which an fft_calculations_complete flag is set. Meanwhile, the latter comprises what is essentially a look-up table to output the dominant frequency as a note that is in human-readable terms. Finally, the DMA1_Channel2_IRQHandler deals with any interrupts that trigger as a result of the SPI-to-DMA-transfer-complete flag being set. More specifically, it waits until the fft_calculations_complete flag is high to copy the contents of the SPI data register (i.e. whatever audio sample the FPGA is sending over at the time) to a second buffer, named input_signal, for frequency_determiner to subsequently process; then, it resets the fft_calculations_complete so that the process may begin anew."
  },
  {
    "objectID": "midpoint_report.html#verification",
    "href": "midpoint_report.html#verification",
    "title": "Midpoint Report",
    "section": "Verification",
    "text": "Verification\nVarious means of verification were used to ensure the project’s accuracy, including the following simulation waveforms, as well as actual experimentation with the hardware itself.\n\nTestbench Simulation\n\n\n\n\n\n\nFigure 4: CIC Filter Testbench\n\n\n\n\n\n\n\n\n\nFigure 5: Halfband Filter Testbench\n\n\n\n\n\n\n\n\n\nFigure 6: FIR Filter Testbench\n\n\n\n\n\n\n\n\n\nFigure 7: Top Level Testbench"
  }
]